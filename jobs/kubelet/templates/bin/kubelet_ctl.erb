#!/bin/bash -ex

NAME="${0##*/}"

source /var/vcap/jobs/kubelet/lib/wait_for_runtime.sh 

export PATH=/var/vcap/packages/kubernetes/bin/:/var/vcap/packages/socat/bin/:$PATH

RUN_DIR=/var/vcap/sys/run/kubernetes
PIDFILE=$RUN_DIR/kubelet.pid
LOG_DIR=/var/vcap/sys/log/kubelet
DATA_DIR=/var/vcap/store/kubernetes

kubectl="/var/vcap/packages/kubernetes/bin/kubectl --kubeconfig=/var/vcap/jobs/kubelet/config/kubeconfig"

<% iaas = nil %>
<% if_p('cloud-provider') do |cloud_provider| %>
  <% iaas = cloud_provider %>
  cloud_provider="<%= cloud_provider %>"
<% end %>

<% if_link('cloud-provider') do |cloud_provider| %>
    cloud_config="/var/vcap/jobs/kubelet/config/cloud-provider.ini"
    <% cloud_provider.if_p('cloud-provider.gce.service_key') do |service_key| %>
      export GOOGLE_APPLICATION_CREDENTIALS="/var/vcap/jobs/kubelet/config/service_key.json"
    <% end %>
    <% cloud_provider.if_p('cloud-provider.aws.access_key_id') do |access_key_id| %>
    export AWS_ACCESS_KEY_ID="<%= access_key_id %>"
    <% end %>
    <% cloud_provider.if_p('cloud-provider.aws.secret_access_key') do |secret_access_key| %>
    export AWS_SECRET_ACCESS_KEY="<%= secret_access_key %>"
    <% end %>
<% end %>

<%
  labels = ["spec.ip=#{spec.ip}","bosh.id=#{spec.id}"]
  # special case for Azure, when Availability Sets are configured, OpsMan indicates their usage by setting zone="Availability Sets"
  unless iaas == "azure" && "#{spec.az}" == "Availability Sets"
    labels << "bosh.zone=#{spec.az}"
  end

  if iaas=="vsphere"
    labels << "failure-domain.beta.kubernetes.io/zone=#{spec.az}"
    labels << "topology.kubernetes.io/zone=#{spec.az}"
  end

  if_p('k8s-args') do |args|
    custom_labels = args.fetch('node-labels', "").split(",")
    labels = custom_labels + labels
    args.delete('node-labels')
  end

  labels = labels.join(",")
%>

# shellcheck disable=SC1091
. /var/vcap/packages/pid_utils/pid_utils.sh

setup_directories() {
  mkdir -p "$RUN_DIR" "$LOG_DIR" "$DATA_DIR"
  chown -R vcap:vcap "$RUN_DIR" "$LOG_DIR" "$DATA_DIR"
}

send_process_stdout_to_logfile() {
  exec 1>> "$LOG_DIR/$NAME.stdout.log"
}

send_process_stderr_to_logfile() {
  exec 2>> "$LOG_DIR/$NAME.stderr.log"
}

<% if_p('http_proxy') do |http_proxy| %>
export http_proxy=<%= http_proxy %>
export HTTP_PROXY=<%= http_proxy %>
<% end %>
<% if_p('https_proxy') do |https_proxy| %>
export https_proxy=<%= https_proxy %>
export HTTPS_PROXY=<%= https_proxy %>
<% end %>
<% if_p('no_proxy') do |no_proxy| %>
export no_proxy=<%= no_proxy %>
export NO_PROXY=<%= no_proxy %>
<% end %>

delete_stale_drained_node() {
  local node_name
  node_name=$(kubectl --kubeconfig /var/vcap/jobs/kubelet/config/kubeconfig-drain get nodes -o wide -L bosh.id | grep "<%= spec.id %>" | grep "NotReady" | awk '{print $1}')
  if [[ ! -z "${node_name}" ]]; then
    kubectl --kubeconfig /var/vcap/jobs/kubelet/config/kubeconfig-drain delete node "${node_name}" --ignore-not-found
  fi
}

get_hostname_override() {
  if [[ "gce" == "$cloud_provider" ]]; then
    hostname_override=$(curl http://metadata.google.internal/computeMetadata/v1/instance/name -H "Metadata-Flavor: Google")
  elif [[ "azure" == "$cloud_provider" ]]; then
    # K8s 1.9 Azure provider assumes  the hostname == nodename and is required to resolve the VM instance ID
    hostname_override=
  else
    hostname_override=<%= spec.ip %>
  fi

  echo $hostname_override
}

start_kubelet() {
# Previous versions of this script contained a destructive symlinking of a packaged binary called
# `nsenter` because the utiil-linux package did not contain that binary on some Ubuntu release years
# ago that is no longer an issue. So now, we ship the version of `nsenter` as a binary and restore
# the state of the system. This is required in an upgrade scenario. New installs will just have
# the appropriate binary in the appropriate place already.
#
# TODO - Remove this stuff in 1.10
#
  f="/usr/bin/nsenter"
  if [ -L "${f}" ] && [ "$(readlink "${f}")" == "/var/vcap/jobs/kubelet/packages/cni/bin/nsenter" ]; then
    local ret=0
    rm "${f}" && cp "/var/vcap/jobs/kubelet/packages/cni/bin/nsenter" "/usr/bin/" || ret=$?
    if [ "${ret}" != "0" ]; then
      >&2 printf "Error: Unable to replace symlink to old nsenter with actual binary.\n"
      exit 1
    fi
  fi
# -- END 1.9 nsenter hack
  <%-
    include_config = false
    if !iaas.nil? and iaas != "vsphere"
      if_link('cloud-provider') do
        include_config = true
      end
    end
  -%>

  kubelet \
  <%-
    if_p('k8s-args') do |args|
      args.each do |flag, value|
        valueString = ""

        if value.nil?
          # Do nothing to supports args-less flags (--example)
        elsif value.is_a? Array
          valueString = "=#{value.join(",")}"
        elsif value.is_a? Hash
          valueString = "=#{value.map { |k,v| "#{k}=#{v}" }.join(",")}"
        else
          valueString = "=#{value}"
        end
  -%>
    <%= "--#{flag}#{valueString}" %> \
  <%-
      end
    end
  -%>
  <%-
    if_p('file-arguments') do |args|
      args.each do |flag, content|
        fileName = "/var/vcap/jobs/kubelet/config/"+flag
  -%>
    "<%= "--#{flag}=#{fileName}" %>" \
  <%-
      end
    end
  -%>
    <% if include_config -%>--cloud-config=${cloud_config}<% end %> \
    <% if !iaas.nil? -%>--cloud-provider=${cloud_provider}<% end %> \
    --hostname-override=$(get_hostname_override) \
    --node-labels=<%= labels %> \
    --config="/var/vcap/jobs/kubelet/config/kubeletconfig.yml" \
    --tls-cipher-suites=<%= link('kube-apiserver').p('tls-cipher-suites') %> \
  1>> $LOG_DIR/kubelet.stdout.log \
  2>> $LOG_DIR/kubelet.stderr.log
}

stop_kubelet() {
  kill_and_wait "$PIDFILE"
}

pid() {
  head -1 "$PIDFILE"
}

stop_associated_logging_processes() {
  # shellcheck disable=SC2046
  pkill -g $(get_group_pid)
}

get_group_pid() {
  ps -ho pgrp "$(pid)"
}

find_current_ready_worker() {
  local worker_name=$1
  # As long as "o pipefail" is not set, this will result in empty string instead of aborting the script
  # if the node is not ready, which is the behavior we want
  $kubectl get nodes -o wide -L bosh.id | grep "^$worker_name" | grep ' Ready' | awk '{print $1}'
}

remove_network_unavailable_taint() {
  node_name=""
  ready_node_name=""
  taint_wait_duration=0
  while [[ "$taint_wait_duration" -lt 120 && ("$ready_node_name" != "$node_name" || "$node_name" == "") ]]; do
    # sleeping first, to allow the k8s cluster to detect the taint
    taint_wait_duration=$(( taint_wait_duration+20 ))
    sleep 20
    node_name=$($kubectl get nodes -o wide -L bosh.id | grep "<%= spec.id %>$" | awk '{print $1}')
    if [ "$node_name" != "" ]
    then
      echo "Checking for Ready worker '$node_name'"
      ready_node_name=$(find_current_ready_worker $node_name)
    else
      echo "Did not find a node name at $(date), will retry"
    fi
  done

  if [[ "$node_name" == "" ]]
  then
    echo "Was not able to find node with id <%= spec.id %>"
  elif [[ "$ready_node_name" != "$node_name" ]]
  then
    echo "Could not find $node_name listed as Ready, not attempting to remove NetworkUnavailable taint"
  else
    echo "Found $node_name as Ready, checking for NetworkUnavailable taint"
    taint_bool="$($kubectl get nodes ${node_name} -o jsonpath='{.status.conditions[?(@.type=="NetworkUnavailable")].status}')"
    if [ "$taint_bool" ==  "True" ]
    then
      echo "Found NetworkUnavailable taint, attempting to remove it"
      v="$(cat <<-EOF
      {
        "status":{
          "conditions": [
           {
              "type": "NetworkUnavailable",
              "status": "False",
              "reason": "NetworkProvidedByFlannel",
              "message": "Status manually modified by CFCR kubelet post-start"
           }
          ]
        }
      }
EOF
      )"

      KUBE_TOKEN=$($kubectl  config view --raw -o jsonpath='{.users[?(@.name == "kubelet")].user.token}')
      /usr/bin/curl -sS --cacert /var/vcap/jobs/kubelet/config/apiserver-ca.pem \
        -H "Authorization: Bearer ${KUBE_TOKEN}" \
        -H "Accept: application/json" \
        -H "Content-Type: application/strategic-merge-patch+json" \
        -X PATCH \
        -d "$v" \
        https://master.cfcr.internal:8443/api/v1/nodes/${node_name}/status

      echo "Finished patching network availability"
    else
      echo "Did not find a NetworkUnavailable taint"
    fi
  fi
}

case $1 in

  start)

    setup_directories
    send_process_stdout_to_logfile
    send_process_stderr_to_logfile

    wait_for_runtime

  <% if_p('k8s-args.egress-selector-config-file') do |egress_selector_config_file| %>
    # If Antrea, load antrea image after runtime is ready, PKS-4432
    # Learned from /var/vcap/jobs/disk-pressure-watch/bin/watcher.sh
    /var/vcap/jobs/load-antrea-images/bin/post-start
  <% end %>

    pid_guard "$PIDFILE" "Kubelet"

    echo $$ > $PIDFILE

    delete_stale_drained_node
    # Starting this in the background since start_kubelet never returns
    remove_network_unavailable_taint &
    start_kubelet

    ;;

  stop)
    stop_associated_logging_processes
    stop_kubelet
    ;;

  *)
    echo "Usage: $0 {start|stop}"
    ;;

esac

<%
  ############################################################################################################
  # PLEASE KEEP THIS IN SYNC WITH KUBE-APISERVER, KUBE-CONTROLLER-MANAGER, KUBE-SCHEDULER, KUBELET, AND ETCD #
  ############################################################################################################
  def validateK8sArgs()
    if_p('k8s-args') do
      if p('k8s-args').key?('tls-cipher-suites')
        raise "Do not set tls-cipher-suites in k8s-args. 'tls-cipher-suites' is set by default and cannot be changed."
      end
    end
  end

  validateK8sArgs()
  ############
  # END SYNC #
  ############
%>
